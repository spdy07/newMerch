{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:19.034884Z",
     "start_time": "2022-07-10T16:36:18.753918Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:33.440404Z",
     "iopub.status.busy": "2022-07-17T20:35:33.439675Z",
     "iopub.status.idle": "2022-07-17T20:35:33.470947Z",
     "shell.execute_reply": "2022-07-17T20:35:33.469900Z",
     "shell.execute_reply.started": "2022-07-17T20:35:33.440302Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:19.043399Z",
     "start_time": "2022-07-10T16:36:19.035925Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:33.541291Z",
     "iopub.status.busy": "2022-07-17T20:35:33.541001Z",
     "iopub.status.idle": "2022-07-17T20:35:33.572734Z",
     "shell.execute_reply": "2022-07-17T20:35:33.571877Z",
     "shell.execute_reply.started": "2022-07-17T20:35:33.541266Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_CAND = 'datas/candidate_items.csv'\n",
    "PATH_FEAT = 'datas/item_features.csv'\n",
    "PATH_TRAINS = 'datas/train_sessions.csv'\n",
    "PATH_TRAINP = 'datas/train_purchases.csv'\n",
    "PATH_LEADER = 'datas/test_leaderboard_sessions.csv'\n",
    "\n",
    "candidates = pd.read_csv(PATH_CAND)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:19.085208Z",
     "start_time": "2022-07-10T16:36:19.044131Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:33.628609Z",
     "iopub.status.busy": "2022-07-17T20:35:33.626679Z",
     "iopub.status.idle": "2022-07-17T20:35:33.796976Z",
     "shell.execute_reply": "2022-07-17T20:35:33.795761Z",
     "shell.execute_reply.started": "2022-07-17T20:35:33.628581Z"
    }
   },
   "outputs": [],
   "source": [
    "items = pd.read_csv(PATH_FEAT)\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:19.580313Z",
     "start_time": "2022-07-10T16:36:19.086303Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:33.799390Z",
     "iopub.status.busy": "2022-07-17T20:35:33.799038Z",
     "iopub.status.idle": "2022-07-17T20:35:35.283461Z",
     "shell.execute_reply": "2022-07-17T20:35:35.282355Z",
     "shell.execute_reply.started": "2022-07-17T20:35:33.799353Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def dateparse(date):    \n",
    "    return int(datetime.strptime(date.split('.')[0], '%Y-%m-%d  %H:%M:%S').timestamp())\n",
    "\n",
    "train_purchases = pd.read_csv(PATH_TRAINP, parse_dates=[2])\n",
    "\n",
    "# train_purchases['ts'] = train_purchases['date'].astype('int64')\n",
    "train_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:35.286423Z",
     "iopub.status.busy": "2022-07-17T20:35:35.286014Z",
     "iopub.status.idle": "2022-07-17T20:35:36.450730Z",
     "shell.execute_reply": "2022-07-17T20:35:36.449547Z",
     "shell.execute_reply.started": "2022-07-17T20:35:35.286385Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(PATH_TRAINP, parse_dates=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:21.818892Z",
     "start_time": "2022-07-10T16:36:19.580986Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:36.454125Z",
     "iopub.status.busy": "2022-07-17T20:35:36.452575Z",
     "iopub.status.idle": "2022-07-17T20:35:43.118757Z",
     "shell.execute_reply": "2022-07-17T20:35:43.117794Z",
     "shell.execute_reply.started": "2022-07-17T20:35:36.454084Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sessions = pd.read_csv(PATH_TRAINS, parse_dates=[2])\n",
    "train_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:23.061078Z",
     "start_time": "2022-07-10T16:36:21.819589Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:43.122297Z",
     "iopub.status.busy": "2022-07-17T20:35:43.121898Z",
     "iopub.status.idle": "2022-07-17T20:35:44.541905Z",
     "shell.execute_reply": "2022-07-17T20:35:44.540814Z",
     "shell.execute_reply.started": "2022-07-17T20:35:43.122257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add rank column\n",
    "train_sessions[\"rank\"] = train_sessions.groupby('session_id')['date'].rank(method=\"first\", ascending=False)\n",
    "train_sessions = train_sessions[train_sessions[\"rank\"] <= 10]\n",
    "train_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:23.162850Z",
     "start_time": "2022-07-10T16:36:23.061738Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:44.544235Z",
     "iopub.status.busy": "2022-07-17T20:35:44.543553Z",
     "iopub.status.idle": "2022-07-17T20:35:44.803926Z",
     "shell.execute_reply": "2022-07-17T20:35:44.802910Z",
     "shell.execute_reply.started": "2022-07-17T20:35:44.544194Z"
    }
   },
   "outputs": [],
   "source": [
    "test_leaderboard_sessions = pd.read_csv(PATH_LEADER)\n",
    "test_leaderboard_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding items\n",
    "Items are represented in a sparse format with the id of the feature and its value. We have no information on whether or not it is categorical or numerical, or the number of features. We want to explore them a little bit and find a compact representation.\n",
    "\n",
    "First we want to know the coverage of each feature. Do a plot with, in x-axis, the feature id, and in y-axis, the number of non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:23.479744Z",
     "start_time": "2022-07-10T16:36:23.163526Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:44.805980Z",
     "iopub.status.busy": "2022-07-17T20:35:44.805587Z",
     "iopub.status.idle": "2022-07-17T20:35:44.811690Z",
     "shell.execute_reply": "2022-07-17T20:35:44.810559Z",
     "shell.execute_reply.started": "2022-07-17T20:35:44.805942Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:23.686678Z",
     "start_time": "2022-07-10T16:36:23.480528Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:44.814055Z",
     "iopub.status.busy": "2022-07-17T20:35:44.813293Z",
     "iopub.status.idle": "2022-07-17T20:35:45.555250Z",
     "shell.execute_reply": "2022-07-17T20:35:45.553248Z",
     "shell.execute_reply.started": "2022-07-17T20:35:44.814014Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_id_count = items.groupby(['feature_category_id']).size().reset_index(name='counts')\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "ax.bar(feature_id_count['feature_category_id'], feature_id_count['counts'])\n",
    "ax.set_xticks(np.arange(1, feature_id_count['feature_category_id'].max() + 1, step=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73 features is not that much, let's see how each of them look. Display the counts / min / max / std for each feature. You should see that one feature is useless, remove it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:23.739222Z",
     "start_time": "2022-07-10T16:36:23.688442Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:45.557317Z",
     "iopub.status.busy": "2022-07-17T20:35:45.556665Z",
     "iopub.status.idle": "2022-07-17T20:35:45.728528Z",
     "shell.execute_reply": "2022-07-17T20:35:45.727421Z",
     "shell.execute_reply.started": "2022-07-17T20:35:45.557276Z"
    }
   },
   "outputs": [],
   "source": [
    "items.groupby(\"feature_category_id\")[\"feature_value_id\"].describe().sort_values(by=[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:23.749733Z",
     "start_time": "2022-07-10T16:36:23.739849Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:45.730561Z",
     "iopub.status.busy": "2022-07-17T20:35:45.729973Z",
     "iopub.status.idle": "2022-07-17T20:35:45.750945Z",
     "shell.execute_reply": "2022-07-17T20:35:45.749949Z",
     "shell.execute_reply.started": "2022-07-17T20:35:45.730517Z"
    }
   },
   "outputs": [],
   "source": [
    "items = items.drop(items[items.feature_category_id == 27].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If 73 features is not a lot, it is still a big number given the number of samples for your poor laptop if you make it dense. It is also good practice to preprocess the data into a set of embeddings without sparse format because most ML algorithm do not handle sparse. We want to do it now. Use TruncatedSVD from scikit-learn with n_components = 12 (or less depending on your memory) and compute embeddings for your items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:23.972512Z",
     "start_time": "2022-07-10T16:36:23.750528Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:45.755476Z",
     "iopub.status.busy": "2022-07-17T20:35:45.755186Z",
     "iopub.status.idle": "2022-07-17T20:35:46.878771Z",
     "shell.execute_reply": "2022-07-17T20:35:46.877725Z",
     "shell.execute_reply.started": "2022-07-17T20:35:45.755449Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "\n",
    "items_matrix = coo_matrix((items[\"feature_value_id\"], (items[\"item_id\"], items[\"feature_category_id\"])))\n",
    "items_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:24.084024Z",
     "start_time": "2022-07-10T16:36:23.973212Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:46.880877Z",
     "iopub.status.busy": "2022-07-17T20:35:46.880251Z",
     "iopub.status.idle": "2022-07-17T20:35:47.277884Z",
     "shell.execute_reply": "2022-07-17T20:35:47.276789Z",
     "shell.execute_reply.started": "2022-07-17T20:35:46.880816Z"
    }
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=12)\n",
    "svd.fit(items_matrix)\n",
    "df = pd.DataFrame(svd.transform(items_matrix))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:24.086433Z",
     "start_time": "2022-07-10T16:36:24.084729Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:47.281879Z",
     "iopub.status.busy": "2022-07-17T20:35:47.281541Z",
     "iopub.status.idle": "2022-07-17T20:35:47.288741Z",
     "shell.execute_reply": "2022-07-17T20:35:47.286873Z",
     "shell.execute_reply.started": "2022-07-17T20:35:47.281830Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"item_id\"] = df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a matrix of dimension (28144, n_components) representing the items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:24.092982Z",
     "start_time": "2022-07-10T16:36:24.086974Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:47.290959Z",
     "iopub.status.busy": "2022-07-17T20:35:47.290577Z",
     "iopub.status.idle": "2022-07-17T20:35:47.317013Z",
     "shell.execute_reply": "2022-07-17T20:35:47.315901Z",
     "shell.execute_reply.started": "2022-07-17T20:35:47.290923Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding sessions\n",
    "\n",
    "Sessions are defined by a series of items, we first want to see what they look like to be able to find a suitable representation for them.\n",
    "\n",
    "My advice: start by sampling the train_sessions dataframe to keep 5% or 10% otherwise your memory may suffer.\n",
    "\n",
    "Then plot the distribution of session sizes: x-axis is the size of the session, y-axis is the number of sessions of this size. Determine a threshold to contain completely 90% of the sessions. This is how we will compute the session representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $k$ be the length you have picked for your session. Create a vectorize representation of your sessions by either concatenating the embedding of the corresponding items, or averaging them (if memory is scarse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:47.319288Z",
     "iopub.status.busy": "2022-07-17T20:35:47.318674Z",
     "iopub.status.idle": "2022-07-17T20:35:47.328143Z",
     "shell.execute_reply": "2022-07-17T20:35:47.327201Z",
     "shell.execute_reply.started": "2022-07-17T20:35:47.319248Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_rank(sessions):\n",
    "    sessions[\"rank\"] = sessions.groupby('session_id')['date'].rank(method=\"first\", ascending=False)\n",
    "    sessions = sessions[sessions[\"rank\"] <= 10]\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:47.330310Z",
     "iopub.status.busy": "2022-07-17T20:35:47.329463Z",
     "iopub.status.idle": "2022-07-17T20:35:47.341263Z",
     "shell.execute_reply": "2022-07-17T20:35:47.340358Z",
     "shell.execute_reply.started": "2022-07-17T20:35:47.330271Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding_session(sessions):\n",
    "    sample = sessions.sample(frac=0.10, random_state=42)\n",
    "    distribution = sample.groupby('session_id').size().reset_index(name='counts')\n",
    "    distribution = distribution.groupby('counts').size().reset_index(name='number')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (20,10))\n",
    "    ax.bar(distribution['counts'], distribution['number'])\n",
    "    ax.set_xticks(np.arange(1, distribution['counts'].max() + 1, step=1))\n",
    "    plt.show()\n",
    "    \n",
    "    k = 12\n",
    "    merged = df.merge(sample, on=\"item_id\")\n",
    "    sample = merged.pivot(index=\"session_id\", columns='rank', values=[i for i in range (k)]).fillna(0)\n",
    "    sample.columns = sample.columns.to_flat_index()\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:47.344414Z",
     "iopub.status.busy": "2022-07-17T20:35:47.344049Z",
     "iopub.status.idle": "2022-07-17T20:35:48.917653Z",
     "shell.execute_reply": "2022-07-17T20:35:48.916406Z",
     "shell.execute_reply.started": "2022-07-17T20:35:47.344378Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = embedding_session(train_sessions)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:48.919692Z",
     "iopub.status.busy": "2022-07-17T20:35:48.919122Z",
     "iopub.status.idle": "2022-07-17T20:35:49.736672Z",
     "shell.execute_reply": "2022-07-17T20:35:49.735219Z",
     "shell.execute_reply.started": "2022-07-17T20:35:48.919653Z"
    }
   },
   "outputs": [],
   "source": [
    "test_sessions = add_rank(test_leaderboard_sessions)\n",
    "test_sessions = embedding_session(test_sessions)\n",
    "test_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding purchases\n",
    "Purchases are one item, so we simply use their embedding. However, we also need to add negative samples so that our model not only learn on positives (One class SVM can do this, but those models are not known to generalize well...). In order to generate negative samples, shuffle the item_id columns of 4 copies of the purchase array. The original values have a label 1, the copies that have been shuffled have a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:24.933220Z",
     "start_time": "2022-07-10T16:36:24.899265Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:49.742530Z",
     "iopub.status.busy": "2022-07-17T20:35:49.741683Z",
     "iopub.status.idle": "2022-07-17T20:35:49.896205Z",
     "shell.execute_reply": "2022-07-17T20:35:49.895060Z",
     "shell.execute_reply.started": "2022-07-17T20:35:49.742480Z"
    }
   },
   "outputs": [],
   "source": [
    "negative_train_purchases = pd.concat([train_purchases] * 4)\n",
    "negative_train_purchases[\"y\"] = 0\n",
    "train_purchases[\"y\"] = 1\n",
    "all_purchases = pd.concat([train_purchases, negative_train_purchases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:25.819520Z",
     "start_time": "2022-07-10T16:36:24.933859Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:49.902058Z",
     "iopub.status.busy": "2022-07-17T20:35:49.901258Z",
     "iopub.status.idle": "2022-07-17T20:35:51.951108Z",
     "shell.execute_reply": "2022-07-17T20:35:51.949976Z",
     "shell.execute_reply.started": "2022-07-17T20:35:49.901980Z"
    }
   },
   "outputs": [],
   "source": [
    "all_purchases = df.merge(all_purchases, right_on='item_id', left_index=True).drop(['item_id', 'item_id_x', 'item_id_y', 'date'], axis=1)\n",
    "all_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:36:25.826242Z",
     "start_time": "2022-07-10T16:36:25.820699Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:51.953196Z",
     "iopub.status.busy": "2022-07-17T20:35:51.952791Z",
     "iopub.status.idle": "2022-07-17T20:35:51.959401Z",
     "shell.execute_reply": "2022-07-17T20:35:51.958213Z",
     "shell.execute_reply.started": "2022-07-17T20:35:51.953157Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_purchases, negative_train_purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge everything\n",
    "\n",
    "We merge the whole dataset together to fit a model. For each session we concatenate:\n",
    "* The history of the user\n",
    "* The purchase embedding, with the label\n",
    "\n",
    "We will have our training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:51.961410Z",
     "iopub.status.busy": "2022-07-17T20:35:51.960775Z",
     "iopub.status.idle": "2022-07-17T20:35:52.671623Z",
     "shell.execute_reply": "2022-07-17T20:35:52.670530Z",
     "shell.execute_reply.started": "2022-07-17T20:35:51.961371Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test = test_sessions.merge(all_purchases, on='session_id')\n",
    "y_test = df_test['y']\n",
    "X_test = df_test.drop(\"y\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-10T16:54:29.038110Z",
     "start_time": "2022-07-10T16:54:25.086393Z"
    },
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:52.674165Z",
     "iopub.status.busy": "2022-07-17T20:35:52.673656Z",
     "iopub.status.idle": "2022-07-17T20:35:55.679053Z",
     "shell.execute_reply": "2022-07-17T20:35:55.677971Z",
     "shell.execute_reply.started": "2022-07-17T20:35:52.674115Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = sample.merge(all_purchases, on='session_id')\n",
    "y = df_train['y']\n",
    "X = df_train.drop(\"y\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn a model and predict on test\n",
    "\n",
    "This is it. You have now a dataset that you can directly pass to a LogisticRegression. For the testing part, perform your prediction on all candidates and pick the hundred ones that have the highest results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:55.681151Z",
     "iopub.status.busy": "2022-07-17T20:35:55.680723Z",
     "iopub.status.idle": "2022-07-17T20:35:55.686565Z",
     "shell.execute_reply": "2022-07-17T20:35:55.685497Z",
     "shell.execute_reply.started": "2022-07-17T20:35:55.681111Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:55.688772Z",
     "iopub.status.busy": "2022-07-17T20:35:55.688130Z",
     "iopub.status.idle": "2022-07-17T20:35:57.696612Z",
     "shell.execute_reply": "2022-07-17T20:35:57.695553Z",
     "shell.execute_reply.started": "2022-07-17T20:35:55.688733Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:35:57.699086Z",
     "iopub.status.busy": "2022-07-17T20:35:57.698048Z",
     "iopub.status.idle": "2022-07-17T20:36:28.700573Z",
     "shell.execute_reply": "2022-07-17T20:36:28.699396Z",
     "shell.execute_reply.started": "2022-07-17T20:35:57.699046Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_pred_train = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:36:28.702927Z",
     "iopub.status.busy": "2022-07-17T20:36:28.702215Z",
     "iopub.status.idle": "2022-07-17T20:36:28.708919Z",
     "shell.execute_reply": "2022-07-17T20:36:28.707772Z",
     "shell.execute_reply.started": "2022-07-17T20:36:28.702885Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (average_precision_score,\n",
    "                             confusion_matrix,\n",
    "                             ConfusionMatrixDisplay,\n",
    "                             precision_score,\n",
    "                             classification_report,\n",
    "                             recall_score,\n",
    "                             f1_score,\n",
    "                             accuracy_score,\n",
    "                             RocCurveDisplay,\n",
    "                             PrecisionRecallDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:36:28.711390Z",
     "iopub.status.busy": "2022-07-17T20:36:28.710326Z",
     "iopub.status.idle": "2022-07-17T20:36:29.502086Z",
     "shell.execute_reply": "2022-07-17T20:36:29.501079Z",
     "shell.execute_reply.started": "2022-07-17T20:36:28.711347Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred_train)))\n",
    "print('Average precision score: ' + str(average_precision_score(y_test, y_pred_train)))\n",
    "print(classification_report(y_test, y_pred_train))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_train, labels=clf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "disp.ax_.set_title(\"confusion matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going further\n",
    "\n",
    "Your next task is to have an RNN running on this dataset. Take a look at [this notebook](https://github.com/oakfr/intro-to-reco/blob/master/application/part_2/RNN-next-item-prediction.ipynb) and adapt it to your usecase!\n",
    "\n",
    "You are now ready to start the project. Take a look at the packages [surprise](http://surpriselib.com/) and [Microsoft recommenders](https://github.com/microsoft/recommenders) to find models available out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:36:29.507523Z",
     "iopub.status.busy": "2022-07-17T20:36:29.507245Z",
     "iopub.status.idle": "2022-07-17T20:36:31.623348Z",
     "shell.execute_reply": "2022-07-17T20:36:31.622215Z",
     "shell.execute_reply.started": "2022-07-17T20:36:29.507497Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 10000\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X, y, test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:36:31.625937Z",
     "iopub.status.busy": "2022-07-17T20:36:31.624890Z",
     "iopub.status.idle": "2022-07-17T20:36:32.196716Z",
     "shell.execute_reply": "2022-07-17T20:36:32.195539Z",
     "shell.execute_reply.started": "2022-07-17T20:36:31.625889Z"
    }
   },
   "outputs": [],
   "source": [
    "type(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "X_valRNN = X_val.values.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_testRNN = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(X_val.shape)\n",
    "\n",
    "print(X_valRNN.shape)\n",
    "print(X_testRNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:36:32.198906Z",
     "iopub.status.busy": "2022-07-17T20:36:32.198263Z",
     "iopub.status.idle": "2022-07-17T20:36:41.717557Z",
     "shell.execute_reply": "2022-07-17T20:36:41.716562Z",
     "shell.execute_reply.started": "2022-07-17T20:36:32.198859Z"
    }
   },
   "outputs": [],
   "source": [
    "# RNN with keras with X_train as input and y_train as output\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "# load the dataset\n",
    "X_trainRNN = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, input_shape=(X_trainRNN.shape[1:]), return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-17T20:36:41.719313Z",
     "iopub.status.busy": "2022-07-17T20:36:41.718956Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X_trainRNN, y_train, epochs=EPOCHS, validation_data=(X_valRNN, y_val), batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_testRNN, y_test)\n",
    "print('Accuracy: %.2f' % (accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
